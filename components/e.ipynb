{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:116: SyntaxWarning: invalid escape sequence '\\D'\n",
      "<>:116: SyntaxWarning: invalid escape sequence '\\D'\n",
      "C:\\Users\\Aditya\\AppData\\Local\\Temp\\ipykernel_16328\\1220037017.py:116: SyntaxWarning: invalid escape sequence '\\D'\n",
      "  Dataset_file_path = 'E:\\DataScience_projects\\Sentiment_Analysis\\Dataset\\IMDB Dataset.csv'\n",
      "C:\\Users\\Aditya\\AppData\\Local\\Temp\\ipykernel_16328\\1220037017.py:116: SyntaxWarning: invalid escape sequence '\\D'\n",
      "  Dataset_file_path = 'E:\\DataScience_projects\\Sentiment_Analysis\\Dataset\\IMDB Dataset.csv'\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model_trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 122\u001b[0m\n\u001b[0;32m    117\u001b[0m X,y \u001b[38;5;241m=\u001b[39m data_preprocessing(Dataset_file_path)\n\u001b[0;32m    119\u001b[0m X_tv \u001b[38;5;241m=\u001b[39m text_vectorization(X)\n\u001b[1;32m--> 122\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_trainer\u001b[49m(X_tv,y)\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy of the model is : \u001b[39m\u001b[38;5;124m\"\u001b[39m, accuracy)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model_trainer' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pickle\n",
    "\n",
    "\n",
    "def text_preprocessing(text):\n",
    "\n",
    "        #Remove html tags\n",
    "        pattern = re.compile('<.*?>')\n",
    "        text =  pattern.sub(r'', text)\n",
    "    \n",
    "        #Remove urls\n",
    "        #text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "        temp = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "        text =  temp.sub('', text)\n",
    "\n",
    "        #Lowering the text\n",
    "        text = text.lower()\n",
    "\n",
    "        #Removing punctions\n",
    "        '''\n",
    "        exclude = string.punctuation\n",
    "        text =  text.translate(str.maketrans('','',exclude))\n",
    "        '''\n",
    "        text =  text.translate(str.maketrans('','',string.punctuation))\n",
    "\n",
    "        #Tokanizing text\n",
    "        text = word_tokenize(text)\n",
    "\n",
    "        #Removinng stopwords\n",
    "        stopwords_to_remove = stopwords.words('english')\n",
    "        text = [word for word in text if word not in stopwords_to_remove]\n",
    "        '''\n",
    "        for w in text.split():\n",
    "            if w in stopwords_to_remove:\n",
    "                text = ''.join(text.replace(w, ''))\n",
    "        '''\n",
    "\n",
    "        #Lemmatization\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        text = ' '.join([lemmatizer.lemmatize(word) for word in text])\n",
    "\n",
    "\n",
    "        if not os.path.exists('artifacts'): \n",
    "             os.makedirs('artifacts')\n",
    "        with open('artifacts/preprocessing.pkl','wb')as f:\n",
    "            pickle.dump((text_preprocessing),f)\n",
    "\n",
    "        return text\n",
    "\n",
    "\n",
    "def text_vectorization(X):\n",
    "    tv = TfidfVectorizer( max_features=5000)\n",
    "    X_tv = pd.DataFrame(tv.fit_transform(X).toarray())\n",
    "\n",
    "    if not os.path.exists('artifacts'):\n",
    "        os.makedirs('artifacts')\n",
    "\n",
    "    with open('artifacts/vectorizer.pkl','wb') as f:\n",
    "         pickle.dump((tv), f)\n",
    "\n",
    "    return X_tv\n",
    "\n",
    "\n",
    "def data_preprocessing(Dataset_file_path):\n",
    "    \n",
    "    df = pd.read_csv(Dataset_file_path)\n",
    "    '''\n",
    "    df['review']=df['review'].apply(text_preprocessing)\n",
    "    '''\n",
    "\n",
    "    # Initialize an empty list to store processed chunks\n",
    "    chunks = []\n",
    "    # Process the dataframe in chunks of 10,000 rows\n",
    "    chunk_size = 10000\n",
    "    for start in range(0, len(df), chunk_size):\n",
    "        end = start + chunk_size\n",
    "        chunk = df.iloc[start:end].copy()  # Copy the chunk to avoid modifying the original dataframe\n",
    "        chunk['review'] = chunk['review'].apply(text_preprocessing)\n",
    "        chunks.append(chunk)\n",
    "\n",
    "    # Concatenate the processed chunks back into a single dataframe\n",
    "    df= pd.concat(chunks, ignore_index=True)\n",
    "    # Now df contains the preprocessed reviews\n",
    "\n",
    "\n",
    "    with open('artifacts/preprocessing.pkl','wb')as f:\n",
    "            pickle.dump((text_preprocessing),f)\n",
    "\n",
    "    \n",
    "    X = df['review']\n",
    "    y = df['sentiment']\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    y = le.fit_transform(y)\n",
    "    return X,y\n",
    "\n",
    "\n",
    "    \n",
    " \n",
    "\n",
    "Dataset_file_path = 'E:\\DataScience_projects\\Sentiment_Analysis\\Dataset\\IMDB Dataset.csv'\n",
    "X,y = data_preprocessing(Dataset_file_path)\n",
    "\n",
    "X_tv = text_vectorization(X)\n",
    "\n",
    "\n",
    "accuracy = model_trainer(X_tv,y)\n",
    "print(\"Accuracy of the model is : \", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of continuous-multioutput and binary targets",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score, confusion_matrix, precision_score\n\u001b[1;32m----> 2\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m \u001b[43maccuracy_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_tv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:231\u001b[0m, in \u001b[0;36maccuracy_score\u001b[1;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[0;32m    229\u001b[0m xp, _, device \u001b[38;5;241m=\u001b[39m get_namespace_and_device(y_true, y_pred, sample_weight)\n\u001b[0;32m    230\u001b[0m \u001b[38;5;66;03m# Compute accuracy for each possible representation\u001b[39;00m\n\u001b[1;32m--> 231\u001b[0m y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    232\u001b[0m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[0;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\_classification.py:112\u001b[0m, in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m    109\u001b[0m     y_type \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(y_type) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 112\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    113\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClassification metrics can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt handle a mix of \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m targets\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    114\u001b[0m             type_true, type_pred\n\u001b[0;32m    115\u001b[0m         )\n\u001b[0;32m    116\u001b[0m     )\n\u001b[0;32m    118\u001b[0m \u001b[38;5;66;03m# We can't have more than one value on y_type => The set is no more needed\u001b[39;00m\n\u001b[0;32m    119\u001b[0m y_type \u001b[38;5;241m=\u001b[39m y_type\u001b[38;5;241m.\u001b[39mpop()\n",
      "\u001b[1;31mValueError\u001b[0m: Classification metrics can't handle a mix of continuous-multioutput and binary targets"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score\n",
    "accuracy = accuracy_score(X_tv, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model is :  0.8859\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import pickle\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pickle\n",
    "\n",
    "\n",
    "def text_preprocessing(text):\n",
    "\n",
    "        #Remove html tags\n",
    "        pattern = re.compile('<.*?>')\n",
    "        text =  pattern.sub(r'', text)\n",
    "    \n",
    "        #Remove urls\n",
    "        #text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "        temp = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "        text =  temp.sub('', text)\n",
    "\n",
    "        #Lowering the text\n",
    "        text = text.lower()\n",
    "\n",
    "        #Removing punctions\n",
    "        '''\n",
    "        exclude = string.punctuation\n",
    "        text =  text.translate(str.maketrans('','',exclude))\n",
    "        '''\n",
    "        text =  text.translate(str.maketrans('','',string.punctuation))\n",
    "\n",
    "        #Tokanizing text\n",
    "        text = word_tokenize(text)\n",
    "\n",
    "        #Removinng stopwords\n",
    "        stopwords_to_remove = stopwords.words('english')\n",
    "        text = [word for word in text if word not in stopwords_to_remove]\n",
    "        '''\n",
    "        for w in text.split():\n",
    "            if w in stopwords_to_remove:\n",
    "                text = ''.join(text.replace(w, ''))\n",
    "        '''\n",
    "\n",
    "        #Lemmatization\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        text = ' '.join([lemmatizer.lemmatize(word) for word in text])\n",
    "\n",
    "\n",
    "        if not os.path.exists('artifacts'): \n",
    "             os.makedirs('artifacts')\n",
    "        with open('artifacts/preprocessing.pkl','wb')as f:\n",
    "            pickle.dump((text_preprocessing),f)\n",
    "\n",
    "        return text\n",
    "\n",
    "\n",
    "def text_vectorization(X):\n",
    "    tv = TfidfVectorizer( max_features=5000)\n",
    "    X_tv = pd.DataFrame(tv.fit_transform(X).toarray())\n",
    "\n",
    "    if not os.path.exists('artifacts'):\n",
    "        os.makedirs('artifacts')\n",
    "\n",
    "    with open('artifacts/vectorizer.pkl','wb') as f:\n",
    "         pickle.dump((tv), f)\n",
    "\n",
    "    return X_tv\n",
    "\n",
    "\n",
    "def data_preprocessing(Dataset_file_path):\n",
    "    \n",
    "    df = pd.read_csv(Dataset_file_path)\n",
    "    '''\n",
    "    df['review']=df['review'].apply(text_preprocessing)\n",
    "    '''\n",
    "\n",
    "    # Initialize an empty list to store processed chunks\n",
    "    chunks = []\n",
    "    # Process the dataframe in chunks of 10,000 rows\n",
    "    chunk_size = 10000\n",
    "    for start in range(0, len(df), chunk_size):\n",
    "        end = start + chunk_size\n",
    "        chunk = df.iloc[start:end].copy()  # Copy the chunk to avoid modifying the original dataframe\n",
    "        chunk['review'] = chunk['review'].apply(text_preprocessing)\n",
    "        chunks.append(chunk)\n",
    "\n",
    "    # Concatenate the processed chunks back into a single dataframe\n",
    "    df= pd.concat(chunks, ignore_index=True)\n",
    "    # Now df contains the preprocessed reviews\n",
    "\n",
    "\n",
    "    with open('artifacts/preprocessing.pkl','wb')as f:\n",
    "            pickle.dump((text_preprocessing),f)\n",
    "\n",
    "    \n",
    "    X = df['review']\n",
    "    y = df['sentiment']\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    y = le.fit_transform(y)\n",
    "    return X,y\n",
    "\n",
    "def model_trainer(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    if not os.path.exists(\"artifacts\"):\n",
    "        os.makedirs(\"artifacts\")\n",
    "\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    predicted_sentiment = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, predicted_sentiment)\n",
    "    \n",
    "    # Save the model to a file\n",
    "    with open(\"artifacts/model.pkl\", \"wb\") as f:\n",
    "        pickle.dump(model, f)\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "    \n",
    " \n",
    "\n",
    "Dataset_file_path = 'E:/DataScience_projects/Sentiment_Analysis/Dataset/IMDB Dataset.csv'\n",
    "X,y = data_preprocessing(Dataset_file_path)\n",
    "X_tv = text_vectorization(X)\n",
    "\n",
    "accuracy = model_trainer(X_tv,y)\n",
    "print(\"Accuracy of the model is : \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aditya\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator TfidfTransformer from version 1.2.2 when using version 1.5.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "C:\\Users\\Aditya\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator TfidfVectorizer from version 1.2.2 when using version 1.5.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "C:\\Users\\Aditya\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator LogisticRegression from version 1.2.2 when using version 1.5.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      " * Restarting with watchdog (windowsapi)\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aditya\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask\n",
    "from flask import render_template, redirect,  request\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "app = Flask(__name__)\n",
    "#Load the model\n",
    "preprocessed = pickle.load(open(r\"E:/DataScience_projects/Sentiment_Analysis/components/artifacts/preprocessing.pkl\",\"rb\"))\n",
    "tfidf = pickle.load(open(r\"E:/DataScience_projects/Sentiment_Analysis/components/artifacts/vectorizer.pkl\",\"rb\"))\n",
    "model = pickle.load(open(r\"E:/DataScience_projects/Sentiment_Analysis/components/artifacts/model.pkl\",\"rb\"))\n",
    "\n",
    "\n",
    "\n",
    "@app.route('/', methods=[\"GET\",\"POST\"])\n",
    "def home():\n",
    "    if request.method == \"POST\":\n",
    "        review = request.form['text']\n",
    "        transformed_review = preprocessed.text_preprocessing(review)\n",
    "        vectorized_review = tfidf.tv(transformed_review)\n",
    "        result = model.predict(vectorized_review)\n",
    "\n",
    "        if result == 1: \n",
    "            return render_template('index.html', prediction = \"Positive\")\n",
    "        else:\n",
    "            return render_template('index.html',prediction=\"Negative\")\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    app.run(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
